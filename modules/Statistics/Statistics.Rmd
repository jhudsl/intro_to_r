---
title: "Statistics"
output:
  ioslides_presentation:
    css: ../../docs/styles.css
    widescreen: yes
---

```{r knit-setup, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 4,
  fig.width = 7,
  comment = ""
)
library(dplyr)
options(scipen = 999)
library(readr)
library(ggplot2)
library(emo)

install.packages(c("faraway", "epitools"))
```

## Summary

- `ggplot()` specifies what data use and what variables will be mapped to where
- inside `ggplot()`, `mapping = aes(x = , y = , color =)` specify what variables correspond to what aspects of the plot in general
- layers of plots can be combined using the `+` at the **end** of lines
- use `geom_line()` and `geom_point()` to add lines and points
- sometimes you need to add a `group` element to `mapping = aes()` if your plot looks strange
- make sure you are plotting what you think you are by checking the numbers!
- `facet_grid(~ variable)` and `facet_wrap(~variable)` can be helpful to quickly split up your plot

## Summary

- the factor class allows us to have a different order from alphanumeric for categorical data
- we can change data to be a factor variable using `mutate` , the `as_factor()` (of `forcats` package) or `factor()` function and specifying the levels with the `levels` argument
- the `fct_reorder({variable_to_reorder}, {variable_to_order_by})` helps us reorder a variable by the values of another variable
- arranging, tabulating, and plotting the data will reflect the new order

## Overview

We will cover how to use R to compute some of basic statistics and fit some basic statistical models. 

* Correlation
* T-test
* Linear Regression / Logistic Regression

<br>

```{r, fig.alt="I was told there would be no math", out.width = "35%", echo = FALSE, fig.show='hold',fig.align='center'}
knitr::include_graphics("https://c.tenor.com/O3x8ywLm380AAAAd/chevy-chase.gif")
```

## Overview

::: {style="color: red;"}
`r emo::ji("rotating_light")` We will focus on how to use R software to do these. We will be glossing over the statistical theory and "formulas" for these tests. Moreover, we do not claim the data we use for demonstration meet assumptions of the methods. `r emo::ji("rotating_light")`
:::

There are plenty of resources online for learning more about these methods, as well as dedicated Biostatistics series (at different advancement levels) at the JHU School of Public Health.

Check out [www.opencasestudies.org](https://www.opencasestudies.org/) for deep dives on some of the concepts covered here.

# Correlation

## Correlation 

Function `cor()` computes correlation in R

```
cor(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))
```
- provide two numeric vectors (arguments `x`, `y`), or
- provide a data.frame / tibble with numeric columns only
- by default, Pearson correlation coefficient is computed

## Correlation {.small}

https://jhudatascience.org/intro_to_r/data/Charm_City_Circulator_Ridership.csv

```{r cor1, comment="", message = FALSE}
library(jhur)
circ <- read_circulator()
head(circ)
```

## Correlation for two vectors

First, we compute correlation by providing two vectors.

Like other functions, if there are `NA`s, you get `NA` as the result.  But if you specify use only the complete observations, then it will give you correlation using the non-missing data. 

```{r}
x <- circ %>% pull(orangeAverage)
y <- circ %>% pull(purpleAverage)
```

```{r}
cor(x, y)
cor(x, y, use = "complete.obs")
```

## Correlation for two vectors with plot

In plot form... 

```{r, fig.width=3, fig.height=3}
circ %>%
  ggplot(aes(x = orangeAverage, y = purpleAverage)) +
  geom_point(size = 0.3)
```

## Correlation for data frame columns

We can compute correlation for all pairs of columns of a data frame / matrix. We typically just say, *"compute correlation matrix"*.

Columns must be all numeric! 

```{r}
circ_subset_Average <- circ %>% select(ends_with("Average"))
dim(circ_subset_Average)

cor_mat <- cor(circ_subset_Average, use = "complete.obs")
cor_mat
```

## Correlation for data frame columns with plot

- Google, *"r correlation matrix plot"*

```{r, fig.width=4, fig.height=4}
library(corrplot)
corrplot(cor_mat, type = "upper", order = "hclust")
```

# T-test

## T-test

The commonly used are: 

- **one-sample t-test** -- used to test mean of a variable in one group 
- **two-sample t-test** -- used to test difference in means of a variable between two groups (if the "two groups" are data of the *same* individuals collected at 2 time points, we say it is two-sample paired t-test)

The `t.test()` function in R is one to address the above. 

```
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)
```

## Running one-sample t-test

It tests mean of a variable in one group. By default (i.e., without us explicitly specifying values of other arguments): 

- tests whether a mean of a variable is equal to 0 (`mu=0`) 
- uses "two sided" alternative (`alternative = "two.sided"`) 
- returns result assuming confidence level 0.95 (`conf.level = 0.95`)

```{r}
x <- circ %>% pull(orangeAverage)
t.test(x)
```

## Running two-sample t-test{.small}

It tests test difference in means of a variable between two groups. By default:

- tests whether difference in means of a variable is equal to 0 (`mu=0`) 
- uses "two sided" alternative (`alternative = "two.sided"`) 
- returns result assuming confidence level 0.95 (`conf.level = 0.95`)
- assumes data are not paired (`paired = FALSE`)
- assumes true variance in the two groups is not equal (`var.equal = FALSE`)

```{r}
x <- circ %>% pull(orangeAverage)
y <- circ %>% pull(purpleAverage)
t.test(x, y)
```

## T-test: retrieving information from the result with `broom` package 

The `broom` package has a `tidy()` function that can organize results into a data frame so that they are easily manipulated (or nicely printed)

```{r broom, comment=""}
library(broom)

result <- t.test(x, y)
result_tidy <- tidy(result)
result_tidy
```

## P-value adjustment

`r emo::ji("rotating_light")` You run an increased risk of Type I errors (a "false positive") when multiple hypotheses are tested simultaneously.

Use the `p.adjust()` function on a vector of p values. Use `method = ` to specify the adjustment method:

```{r}
my_pvalues <- c(0.049, 0.001, 0.31, 0.00001)
p.adjust(my_pvalues, method = "BH") # Benjamini Hochberg
p.adjust(my_pvalues, method = "bonferroni")
```

## Some other statistical tests

- `wilcox.test()` -- Wilcoxon signed rank test, Wilcoxon rank sum test
- `shapiro.test()` -- Shapiro test
- `ks.test()` -- Kolmogorov-Smirnov test 
- `var.test()`-- Fisher‚Äôs F-Test
- `chisq.test()` -- Chi-squared test
- `aov()` -- Analysis of Variance (ANOVA)

## Lab Part 1

üè† [Class Website](https://jhudatascience.org/intro_to_r/)  

üíª [Lab](https://jhudatascience.org/intro_to_r/modules/Statistics/lab/Statistics_Lab.Rmd)

# Regression

## Linear regression

Linear regression is a method to model the relationship between a response and one or more explanatory variables. 

We provide a little notation here so some of the commands are easier to put in the proper context.

$$
y_i = \alpha + \beta x_{i} + \varepsilon_i 
$$
where:

* $y_i$ is the outcome for person i
* $\alpha$ is the intercept
* $\beta$ is the slope 
* $x_i$ is the predictor for person i
* $\varepsilon_i$ is the residual variation for person i

## Linear regression

```{r,echo=FALSE}
coefs <- lm(data = iris, Petal.Width ~ Petal.Length) %>% coef()

line_df <- data.frame(x = c(0, 7), y = c(coefs[1], (coefs[2] * 7 + coefs[1])))
riserun_df <- data.frame(x = c(2.1, 2.9, 2.9), y = c((coefs[1] + coefs[2] * 2.1), (coefs[1] + coefs[2] * 2.1), (coefs[1] + coefs[2] * 2.9)))
residual_df <- data.frame(x = c(3, 3), y = c(1.1, (coefs[1] + coefs[2] * 3)))
labels_df <- data.frame(x = c(0.5, 3.2, 2.7), y = c(coefs[1], 0.65, 1), labels = c("alpha", "beta", "epsilon[i]"))

print(ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point() +
  geom_line(data = line_df, aes(x = x, y = y)) +
  geom_point(data = line_df[1, ], aes(x = x, y = y, color = "red", size = 1.5)) +
  geom_line(data = riserun_df, aes(x = x, y = y, color = "orange", size = 1.4)) +
  geom_line(data = residual_df, aes(x = x, y = y, color = "blue", size = 1.4)) +
  geom_label(data = labels_df, aes(x = x, y = y, label = labels, size = 2), parse = TRUE) +
  theme_bw() +
  ylab(expression(y)) +
  xlab(expression(x)) +
  theme(legend.position = "none", text = element_text(size = 20)))
```

## Linear regression

Linear regression is a method to model the relationship between a response and one or more explanatory variables. 

We provide a little notation here so some of the commands are easier to put in the proper context.

$$
y_i = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \varepsilon_i 
$$
where:

* $y_i$ is the outcome for person i
* $\alpha$ is the intercept
* $\beta_1$, $\beta_2$, $\beta_2$ are the slopes for variables $x_{i1}$, $x_{i2}$, $x_{i3}$ 
* $x_{i1}$, $x_{i2}$, $x_{i3}$ are the predictors for person i
* $\varepsilon_i$ is the residual variation for person i

## Linear regression fit in R 

To fit regression models in R, we use the function `glm()` (Generalized Linear Model). 

We typically provide two arguments: 

- `formula` -- model formula written using names of columns in our data 
- `data` -- our data frame 

## Linear regression fit in R: model formula 

Model formula 
$$
y_i = \alpha + \beta x_{i} + \varepsilon_i 
$$
In R translates to 

```{r}

```

<p style="text-align: center;">
`y ~ x`
</p>

## Linear regression fit in R: model formula 

Model formula 
$$
y_i = \alpha + \beta x_{i} + \varepsilon_i 
$$
In R translates to 

<p style="text-align: center;">
`y ~ x`
</p>

In practice, `y` and `x` are replaced with the **names of columns from our data set**. 

For example, if we want to fit a regression model where outcome is `income` and predictor is `years_of_education`, our formula would be: 

<p style="text-align: center;">
`income ~ years_of_education`
</p>

## Linear regression fit in R: model formula 

Model formula 
$$
y_i = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \varepsilon_i 
$$
In R translates to 

<p style="text-align: center;">
`y ~ x1 + x2 + x3`
</p>

In practice, `y` and `x1`, `x2`, `x3` are replaced with the **names of columns from our data set**. 

For example, if we want to fit a regression model where outcome is `income` and predictors are `years_of_education`, `age`, `location` then our formula would be: 

<p style="text-align: center;">
`income ~ years_of_education + age + location`
</p>

## Linear regression 

We will use data about emergency room doctor complaints. 

"Data was recorded on 44 doctors working in an emergency service at a hospital to study the factors affecting the number of complaints received."

```{r}
# install.packages("faraway")
library(faraway)

data(esdcomp)
esdcomp
```

## Linear regression: model fitting 

We fit linear regression model with the number of patient visits (`visits`) as an outcome and total number of hours worked (`hours`) as a predictor. 

```{r}
fit <- glm(visits ~ hours, data = esdcomp)
fit
```

## Linear regression: model summary

The `summary()` function returns a list that shows us some more detail

```{r}
summary(fit)
```

## Linear regression: multiple predictors {.smaller}

Let's try adding another explanatory variable to our model, dollars per hour earned by the doctor (`revenue``).

```{r}
fit2 <- glm(visits ~ hours + revenue, data = esdcomp)
summary(fit2)
```

## Linear regression: factors 

Factors get special treatment in regression models - lowest level of the factor is the comparison group, and all other factors are relative to its values.

`residency` takes values Y or N to indicate whether the doctor is a resident.

```{r}
esdcomp %>% count(residency)
```

## Linear regression: factors {.smaller}

```{r regress6, comment="", fig.height=4,fig.width=8}
fit_3 <- glm(visits ~ residency, data = esdcomp)
summary(fit_3)
```

## Generalized Linear Models (GLMs)

Generalized Linear Models (GLMs) allow for fitting regressions for non-continuous/normal outcomes. Examples include: logistic regression, Poisson regression. 

Add the `family` argument -- a description of the error distribution and link function to be used in the model. These include: 

- `binomial(link = "logit")`
- `poisson(link = "log")`, and other.

See `?family` documentation for details of family functions. 

## Logistic regression {.smaller}

We will use data about breast cancer tumors. 

"The purpose of this study was to determine whether a new procedure called fine needle aspiration which draws only a small sample of tissue could be effective in determining tumor status."

```{r}
data(wbca)
wbca
```

## Logistic regression {.smaller}

`Class` is a 0/1-valued variable indicating if the tumor was malignant (0 if malignant, 1 if benign). 

```{r regress7, comment="", fig.height=4,fig.width=8}
binom_fit <- glm(Class ~ UShap + USize, data = wbca, family = binomial())
summary(binom_fit)
```

## Odds ratios

```{r echo = FALSE}
library(epitools)
data(oswego)
ice_cream <-
  oswego %>%
  select(ill, vanilla.ice.cream) %>%
  mutate(
    ill = recode(ill, "Y" = 1, "N" = 0),
    vanilla.ice.cream = recode(vanilla.ice.cream, "Y" = 1, "N" = 0)
  )
```

This data shows whether people became ill after eating ice cream in the 1940s.

```{r}
head(ice_cream)
ice_cream %>% count(ill, vanilla.ice.cream)
```

## Odds ratios

Use `oddsratio(x, y)` from the `epitools()` package to calculate 

```{r}
library(epitools)
response <- ice_cream %>% pull(ill)
predictor <- ice_cream %>% pull(vanilla.ice.cream)
oddsratio(predictor, response)
```

## Final note 

Some final notes: 

- Researcher's responsibility to **understand the statistical method**  they use -- underlying assumptions, correct interpretation of method results 

- Researcher's responsibility to **understand the R software**  they use -- meaning of function's arguments and meaning of  function's output elements

## Summary

- Use `cor()` to calculate correlation between two vectors. `corrplot()` is nice for a quick visualization!
- `t.test()` tests the difference in means between two vectors
- `glm()` fits regression models:
     - Use the `formula =` argument to specify the model (e.g., `y ~ x` or `y ~ x1 + x2` using column names)
     - Use `data =` to indicate the dataset
     - Use `family = binomial()` to do a logistic regression
     - `summary()` gives useful statistics
- `oddsratio()` from the `epitools` package can calculate odds ratios
- this is just the tip of the iceberg!

## Lab Part 2

üè† [Class Website](https://jhudatascience.org/intro_to_r/)  

üíª [Lab](https://jhudatascience.org/intro_to_r/modules/Statistics/lab/Statistics_Lab.Rmd)

The end!

# Extra Slides

## Adding correlation value to a plot

Note that you can add the correlation value to a plot, via the `annotate()`. 

```{r, fig.width=3, fig.height=3}
cor_val <- cor(x, y, use = "complete.obs")
cor_val_label <- paste0("r = ", round(cor_val, 3))

circ %>%
  ggplot(aes(x = orangeAverage, y = purpleAverage)) +
  geom_point(size = 0.3) +
  annotate("text", x = 2000, y = 7500, label = cor_val_label, size = 5)
```
